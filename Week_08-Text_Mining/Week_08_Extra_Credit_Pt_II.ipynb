{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Dct-Y6XhTCjoomECSSxIZNzmDc2txsfI",
      "authorship_tag": "ABX9TyNkKNu8zaXjw/+C9+zFlFiu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC5810/blob/main/Week_08-Text_Mining/Week_08_Extra_Credit_Pt_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 08 Extra Credit"
      ],
      "metadata": {
        "id": "_8YVQKH72PKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part II"
      ],
      "metadata": {
        "id": "6Yf9zRogjnkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# url = ''\n",
        "# df = pd.read_csv(url)\n",
        "# print(df.shape)\n",
        "# print(df.info())\n",
        "# df.head()"
      ],
      "metadata": {
        "id": "GE7Uylljjg6s"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install faker"
      ],
      "metadata": {
        "id": "XUwiPTugm6xF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # runt this cell if using the dataset prepared in pt 1\n",
        "# import pandas as pd\n",
        "# from faker import Faker\n",
        "\n",
        "# fake = Faker()\n",
        "\n",
        "# def make_name(row):\n",
        "#   return f'{fake.last_name()}, {fake.first_name()}'\n",
        "\n",
        "# df['name'] = df.apply(make_name, axis=1)\n",
        "# print(df.shape)\n",
        "# print(df.info())\n",
        "# df.head()"
      ],
      "metadata": {
        "id": "f51XHFzhm8o3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # detelet missing values\n",
        "# df.dropna(inplace=True)\n",
        "# print(df.shape)\n",
        "# print(df.info())\n",
        "# df.head()"
      ],
      "metadata": {
        "id": "SqnKWPEbpNfc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # train test split\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#                                 df.drop(['survived'], axis=1), # what is your dependent variable?\n",
        "#                                 df['survived'],\n",
        "#                                 test_size=0.25,\n",
        "#                                 random_state=42)\n",
        "\n",
        "# print(X_train.shape)\n",
        "# print(X_test.shape)\n",
        "# X_train.head()"
      ],
      "metadata": {
        "id": "zUDoFjaqpDPZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # https://www.kaggle.com/code/oscarbeijbom/titanictransformers/notebook\n",
        "# # https://www.nyckel.com/blog/titanic-vs-transformers/\n",
        "# # https://www.kaggle.com/code/isobeldaley/titanic-score-distribution-excluding-score-1\n",
        "# import numpy as np\n",
        "# from typing import List\n",
        "# from tqdm import tqdm\n",
        "# from transformers import AutoTokenizer, AutoModel\n",
        "# from xgboost.sklearn import XGBClassifier\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "# model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# def extract(texts: List[str]) -> np.array:\n",
        "#         feats = np.zeros((len(texts), 768), dtype=np.float16)\n",
        "#         for itt, text in enumerate(tqdm(texts)):\n",
        "#             tokenized_text = tokenizer(text, return_tensors='pt')\n",
        "#             model_output = model(**tokenized_text)[0].detach()\n",
        "#             feats[itt, :] = model_output.numpy().mean(axis=1)\n",
        "#         return feats\n",
        "\n",
        "# def make_text(row):\n",
        "#   texts.append(' '.join(row.to_string(index=False).split()))\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# classifier = XGBClassifier(use_label_encoder=False)\n",
        "\n",
        "# train_targets = [target for target in y_train]\n",
        "# texts = []\n",
        "# X_train.apply(make_text, axis=1)\n",
        "# train_features = scaler.fit_transform(extract(texts))\n",
        "# classifier.fit(train_features, train_targets)\n",
        "\n",
        "# test_targets = [target for target in y_test]\n",
        "# texts = []\n",
        "# X_test.apply(make_text, axis=1)\n",
        "# predictions = classifier.predict(scaler.transform(extract(texts)))\n",
        "\n",
        "# print()\n",
        "# print(accuracy_score(test_targets, predictions))\n",
        "# print(confusion_matrix(test_targets, predictions))\n",
        "# print(classification_report(test_targets, predictions))"
      ],
      "metadata": {
        "id": "-QLFTV56pA3W"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}